#Instead of arbitrary values, we now use method "Cross validation" to estimate lambda:
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam # the best lambda
log(bestlam) # log value of previous lambda
# Prediction of the model with the best value of lambda
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
# Prediction of the coefficients with the best value of lambda
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:34,]
# As expected, none of the coefficients is zero
# Figure that show the variation of the coefficients with different values of lambda
dev.new()
plot(out,label = T, xvar = "lambda")
#####################
####### LASSO #######
# Use the argument alpha = 1 to perform Lasso
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T) # L1 norm
# Figure that show the variation of the coefficients with different values of lambda
dev.new()
plot(lasso.mod,label = T, xvar = "lambda")
# Perform Cross-Validation for estimate the best lambda to minimize "mse test"
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min; print(bestlam);print(log(bestlam)) # the best lambda
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2) # 29.1672 slighly larger than ridge
# Comparison between lm() and lasso-model with lambda = 0
lasso.pred=predict(lasso.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:34,]
lasso.coef
lasso.coef[lasso.coef!=0]
cat("Number of coefficients equal to 0:",sum(lasso.coef==0),"\n")
#### Best subset selection and step forward and backward methods ####
# The regsubsets() function (part of the leaps library) performs best subset selection
# by identifying the best model that contains a given number of predictors,
set.seed(3)
dim <- length(coef(lm(poly3.model.noWH)))-1
regfit.full=regsubsets(poly3.model.noWH,df, nvmax = dim)
reg.summary=summary(regfit.full)
reg.summary$rsq # R2 statistic increases monotonically as more variables are included.
which.min(reg.summary$rss) # identify the location of the minimum in the model with 33 predictors.
which.max(reg.summary$adjr2) # in fact Adjusted R2 statistic is max in the model with 25 predictors.
which.min(reg.summary$bic) # BIC is min in the model with 17 predictors
which.min(reg.summary$cp) # Cp is min in the model with 25 predictors
# The best results is the model with 8 predictors
cat("\nLocation of RSS min:",which.min(reg.summary$rss),"\n")
cat("Location of adj-RSq max:",which.max(reg.summary$adjr2),"\n ")
cat("Location of Cp min:",which.min(reg.summary$cp),"\n ")
cat("Location of BIC min:",which.min(reg.summary$bic),"\n ")
# Plot RSS, adjusted R2, Cp, and BIC for all of the models at once
dev.new()
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
points(which.min(reg.summary$rss),min(reg.summary$rss), col="red",cex=2,pch=20)
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")
points(which.max(reg.summary$adjr2),max(reg.summary$adjr2), col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type="l")
points(which.min(reg.summary$cp ),min(reg.summary$cp),col="red",cex=2,pch=20)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(which.min(reg.summary$bic),min(reg.summary$bic),col="red",cex=2,pch=20)
# Another way to see the best model according the graphs
dev.new()
plot(regfit.full,scale="r2")
dev.new()
plot(regfit.full,scale="adjr2")
dev.new()
plot(regfit.full,scale="Cp") # best model with "Cp" min
dev.new()
plot(regfit.full,scale="bic") # best model with smaller "bic"
# The BIC is a variant of the ACI with higher penalty terms. (It is closely related to the ACI)
coef(regfit.full ,which.min(reg.summary$bic)) #see the coefficient estimates for the 8-variable model
####### Forward and Backward Stepwise Selection #######
# Using the argument method="forward" or method="backward".
## Forward
regfit.fwd=regsubsets(poly3.model.noWH,data=df, nvmax=17, method ="forward")
summary(regfit.fwd)
# We see that using forward stepwise selection, the best one-variable is recommended.
## Backward
regfit.bwd=regsubsets(poly3.model.noWH,data=df,nvmax=17, method ="backward")
summary(regfit.bwd)
# The models found by best subset, forward and backward selection are equal.
ii=17; summary(regfit.full)$outmat[ii,]==summary(regfit.fwd)$outmat[ii,]
coef(regfit.full,17)
coef(regfit.fwd,17)
coef(regfit.bwd,17)
# Same results with cleaner output
round(coef(regfit.full,17),3)
round(coef(regfit.fwd,17),3)
round(coef(regfit.bwd,17),3)
## Choosing Among Models Using the Validation Set Approach
####### Validation Set Approach ####
set.seed (2000)
# Sampling with replacement
train=sample(c(TRUE,FALSE), nrow(df),rep=TRUE)
sum(train)
test=(!train)
sum(test)
# Apply best subset selection to the training set
regfit.best=regsubsets(poly3.model.noWH,data=df[train,], nvmax = 17)
# Make a model matrix from the test data.
# The model.matrix() function is used in many regression packages for
# building an "X" matrix from data.
# Using a error matrix to calculate the test error
test.mat=model.matrix(poly3.model.noWH,data=df[test,])
# Now we run a loop, and for each size i, we extract the coefficients
# from regfit.best for the best model of that size,
# multiply them into the appropriate columns of the test model matrix
# to form the predictions, and compute the test MSE.
# Compute the MSE test for best model from 1 to 17 regressors
val.errors=rep(NA,17)
for(i in 1:17){
coefi = coef(regfit.best,id=i)
pred = test.mat[,names(coefi)]%*%coefi
val.errors[i] = mean((df$bmi[test]-pred)^2)
}
# The best model is the one that contains which.min(val.errors) =  17 variables.
val.errors; which.min(val.errors)
coef(regfit.best,which.min(val.errors)) # This is based on training data
# There is not a predict() method for regsubsets().
# Since we will be using this function again, we can capture our steps above and write our own predict method.
predict.regsubsets = function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form, newdata)
coefi=coef(object, id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
#################
###### PCR ######
set.seed (2)
# Use the pcr() function:
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection)
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pcr.fit=pcr(poly4.model.noWH.PCR,data = df ,scale=TRUE, validation ="CV")
#Data: 	X dimension: [29] 2111
#       Y dimension: [1]  2111
# By default, "pcr" gives us the RMSE (Root MSE) in terms of prediction with croos validation approach
# For each component, the result gives us the RMSE, as the number of components changes, and the variance explained.
# First line: variance explained as the number of regressors changes.
# Second line: variance explained as a function of bmi variance.
# Variance explained:
#   - x: 100% when there are all regressors
#   - bmi: 90.71%
summary(pcr.fit)
# Note that pcr() reports the root mean squared error;
# It also provides the percentage of variance explained in the predictors and in the response using different numbers of components.
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# We see that the smallest CV error occurs when M = 29
# This suggests that there is no benefit in terms of reduction of dimensionality (M = 29)
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(poly4.model.noWH.PCR,data = df)[,-1]
y = df$bmi
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
pcr.fit=pcr(poly4.model.noWH.PCR,data = df ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR # M=27 shows the lowest CV error
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2)
# This test set MSE is competitive with the results obtained using ridge and the lasso
# Finally, we fit PCR on the full data set, using M = 27
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
# Plot of the y values of dataset and those predicted by the model
plot(pcr.fit, ncomp = minPCR, asp = 1, line = TRUE)
coef(pcr.fit) ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the pls() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(poly4.model.noWH.PCR,data = df, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 13
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(poly4.model.noWH.PCR,data = df, subset=train,
scale=TRUE, validation ="CV")
dev.new()
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]); # M = 13
pls.pred=predict(pls.fit,x[test,],ncomp=14)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=13)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=12)
mean((pls.pred-y.test)^2) #
# The test MSE is comparable to (slightly higher) the test MSE obtained using ridge regression, the lasso, and PCR.
# Finally, we perform PLS using the full data set, using M = 13
pls.fit=plsr(poly4.model.noWH.PCR,data = df ,scale=TRUE,ncomp=13)
summary(pls.fit)
#### Best subset selection and step forward and backward methods ####
# The regsubsets() function (part of the leaps library) performs best subset selection
# by identifying the best model that contains a given number of predictors,
set.seed(3)
dim <- length(coef(lm(poly3.model.noWH)))-1
regfit.full=regsubsets(poly3.model.noWH,df, nvmax = dim)
reg.summary=summary(regfit.full)
reg.summary$rsq # R2 statistic increases monotonically as more variables are included.
which.min(reg.summary$rss) # identify the location of the minimum in the model with 33 predictors.
which.max(reg.summary$adjr2) # in fact Adjusted R2 statistic is max in the model with 25 predictors.
which.min(reg.summary$bic) # BIC is min in the model with 17 predictors
which.min(reg.summary$cp) # Cp is min in the model with 25 predictors
# The best results is the model with 8 predictors
cat("\nLocation of RSS min:",which.min(reg.summary$rss),"\n")
cat("Location of adj-RSq max:",which.max(reg.summary$adjr2),"\n ")
cat("Location of Cp min:",which.min(reg.summary$cp),"\n ")
cat("Location of BIC min:",which.min(reg.summary$bic),"\n ")
# Plot RSS, adjusted R2, Cp, and BIC for all of the models at once
dev.new()
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
points(which.min(reg.summary$rss),min(reg.summary$rss), col="red",cex=2,pch=20)
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")
points(which.max(reg.summary$adjr2),max(reg.summary$adjr2), col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type="l")
points(which.min(reg.summary$cp ),min(reg.summary$cp),col="red",cex=2,pch=20)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(which.min(reg.summary$bic),min(reg.summary$bic),col="red",cex=2,pch=20)
# Another way to see the best model according the graphs
dev.new()
plot(regfit.full,scale="r2")
dev.new()
plot(regfit.full,scale="adjr2")
dev.new()
plot(regfit.full,scale="Cp") # best model with "Cp" min
dev.new()
plot(regfit.full,scale="bic") # best model with smaller "bic"
# The BIC is a variant of the ACI with higher penalty terms. (It is closely related to the ACI)
coef(regfit.full ,which.min(reg.summary$bic)) #see the coefficient estimates for the 8-variable model
####### Forward and Backward Stepwise Selection #######
# Using the argument method="forward" or method="backward".
## Forward
regfit.fwd=regsubsets(poly3.model.noWH,data=df, nvmax=17, method ="forward")
summary(regfit.fwd)
# We see that using forward stepwise selection, the best one-variable is recommended.
## Backward
regfit.bwd=regsubsets(poly3.model.noWH,data=df,nvmax=17, method ="backward")
summary(regfit.bwd)
# The models found by best subset, forward and backward selection are equal.
ii=17; summary(regfit.full)$outmat[ii,]==summary(regfit.fwd)$outmat[ii,]
coef(regfit.full,17)
coef(regfit.fwd,17)
coef(regfit.bwd,17)
# Same results with cleaner output
round(coef(regfit.full,17),3)
round(coef(regfit.fwd,17),3)
round(coef(regfit.bwd,17),3)
## Choosing Among Models Using the Validation Set Approach
####### Validation Set Approach ####
set.seed (2000)
# Sampling with replacement
train=sample(c(TRUE,FALSE), nrow(df),rep=TRUE)
sum(train)
test=(!train)
sum(test)
# Apply best subset selection to the training set
regfit.best=regsubsets(poly3.model.noWH,data=df[train,], nvmax = 17)
# Make a model matrix from the test data.
# The model.matrix() function is used in many regression packages for
# building an "X" matrix from data.
# Using a error matrix to calculate the test error
test.mat=model.matrix(poly3.model.noWH,data=df[test,])
# Now we run a loop, and for each size i, we extract the coefficients
# from regfit.best for the best model of that size,
# multiply them into the appropriate columns of the test model matrix
# to form the predictions, and compute the test MSE.
val.errors=rep(NA,17)
for(i in 1:17){
coefi = coef(regfit.best,id=i)
pred = test.mat[,names(coefi)]%*%coefi
val.errors[i] = mean((df$bmi[test]-pred)^2)
}
# The best model is the one that contains which.min(val.errors) =  17 variables.
val.errors; which.min(val.errors)
coef(regfit.best,which.min(val.errors)) # This is based on training data
#### Best subset selection and step forward and backward methods ####
# The regsubsets() function (part of the leaps library) performs best subset selection
# by identifying the best model that contains a given number of predictors,
set.seed(3)
dim <- length(coef(lm(poly4.model.noWH)))-1
regfit.full=regsubsets(poly4.model.noWH,df, nvmax=dim)
reg.summary=summary(regfit.full)
reg.summary$rsq # R2 statistic increases monotonically as more variables are included.
which.min(reg.summary$rss) # identify the location of the minimum in the model with 39 predictors
which.max(reg.summary$adjr2) # Adjusted R2 is max in the model with 31 predictors
which.min(reg.summary$bic) # BIC is min in the model with 22 predictors
which.min(reg.summary$cp) # Cp is min in the model with 28 predictors
cat("\nLocation of RSS min:",which.min(reg.summary$rss),"\n")
cat("Location of adj-RSq max:",which.max(reg.summary$adjr2),"\n ")
cat("Location of Cp min:",which.min(reg.summary$cp),"\n ")
cat("Location of BIC min:",which.min(reg.summary$bic),"\n ")
# Plot RSS, adjusted R2, Cp, and BIC for all of the models at once
dev.new()
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
points(which.min(reg.summary$rss),min(reg.summary$rss), col="red",cex=2,pch=20)
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")
points(which.max(reg.summary$adjr2),max(reg.summary$adjr2), col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type="l")
points(which.min(reg.summary$cp ),min(reg.summary$cp),col="red",cex=2,pch=20)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(which.min(reg.summary$bic),min(reg.summary$bic),col="red",cex=2,pch=20)
# Another way to see the best model according the graphs
dev.new()
plot(regfit.full,scale="r2")
dev.new()
plot(regfit.full,scale="adjr2")
dev.new()
plot(regfit.full,scale="Cp") # best model with "Cp" min
dev.new()
plot(regfit.full,scale="bic") # best model with smaller "bic"
# The BIC is a variant of the ACI with higher penalty terms. (It is closely related to the ACI)
coef(regfit.full ,which.min(reg.summary$bic)) #see the coefficient estimates for the 22-variable model
####### Forward and Backward Stepwise Selection #######
# Using the argument method="forward" or method="backward".
## Forward
regfit.fwd=regsubsets(poly4.model.noWH, data=df, nvmax=22, method ="forward")
regfit.fwd.summary = summary(regfit.fwd)
regfit.fwd.summary$rsq # R2 statistic increases monotonically as more variables are included.
which.min(regfit.fwd.summary$rss) # identify the location of the minimum in the model with 29 predictors
which.max(regfit.fwd.summary$adjr2) # Adjusted R2 is max in the model with 25 predictors
dev.new()
plot(regfit.fwd,scale="r2")
dev.new()
plot(regfit.fwd,scale="adjr2")
dev.new()
plot(regfit.fwd,scale="Cp") # best model with "Cp" min
dev.new()
plot(regfit.fwd,scale="bic") # best model with smaller "bic"
## Backward
regfit.bwd=regsubsets(poly4.model.noWH,data=df,nvmax=22, method ="backward")
summary(regfit.bwd)
regfit.bwd.summary = summary(regfit.bwd)
regfit.bwd.summary$rsq # R2 statistic increases monotonically as more variables are included.
which.min(regfit.bwd.summary$rss) # identify the location of the minimum in the model with 22 predictors
which.max(regfit.bwd.summary$adjr2) # Adjusted R2 is max in the model with 22 predictors
which.min(regfit.bwd.summary$bic) # BIC is min in the model with 22 predictors.
dev.new()
plot(regfit.bwd,scale="r2")
dev.new()
plot(regfit.bwd,scale="adjr2")
dev.new()
plot(regfit.bwd,scale="Cp") # best model with "Cp" min
dev.new()
plot(regfit.bwd,scale="bic") # best model with smaller "bic"
#comparison between full forward selection and backward selection
ii=22; summary(regfit.full)$outmat[ii,]==summary(regfit.fwd)$outmat[ii,]
coef(regfit.full,22)
coef(regfit.fwd,22)
coef(regfit.bwd,22)
# Same results with cleaner output
round(coef(regfit.full,22),3)
round(coef(regfit.fwd,22),3)
round(coef(regfit.bwd,22),3)
## Choosing Among Models Using the Validation Set Approach
####### Validation Set Approach ####
set.seed (2000)
# Sampling with replacement
train=sample(c(TRUE,FALSE), nrow(df),rep=TRUE)
sum(train)
test=(!train)
sum(test)
# Apply best subset selection to the training set
regfit.best=regsubsets(poly4.model.noWH,data=df[train,], nvmax =22)
# Make a model matrix from the test data.
# The model.matrix() function is used in many regression packages for
# building an "X" matrix from data.
# Using a error matrix to calculate the test error
test.mat=model.matrix(poly4.model.noWH,data=df[test,])
# Now we run a loop, and for each size i, we extract the coefficients
# from regfit.best for the best model of that size,
# multiply them into the appropriate columns of the test model matrix
# to form the predictions, and compute the test MSE.
val.errors=rep(NA,22)
for(i in 1:22){
coefi = coef(regfit.best,id=i)
pred = test.mat[,names(coefi)]%*%coefi
val.errors[i] = mean((df$bmi[test]-pred)^2)
}
# The best model is the one that contains which.min(val.errors) =  22 variables.
val.errors; which.min(val.errors)
coef(regfit.best,which.min(val.errors)) # This is based on training data
# Linear regression - multiple regression, with regularization
###### REGULARIZATION #######
x = model.matrix(poly4.model.noWH, df)[,-1] #[-1] means no intercept
y = df$bmi
# The model.matrix() function is particularly useful for creating x; not only does it produce a matrix
# corresponding to the 39 predictors but it also automatically transforms any qualitative
# variables into dummy variables (recommended).
# The latter property is important because glmnet() can only take numerical, quantitative inputs.
########################
######### Ridge ########
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^-2 to 10^10)
# Inputs of glmnet: regressors matrix, dependent variable and alpha value:
# alpha = 0 (ridge regression), alpha = 1 (lasso)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod)) # 40 coefficients, 100 lambda values
# We expect the coefficient estimates to be much smaller, in terms of l2 norm, when a large value of lambda is used, as
# compared to when a small value is used.
# grid[50]
ridge.mod$lambda[50]
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2))
# grid[60]
ridge.mod$lambda[60]
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2))
# As lambda decreases -> the coefficients increase. As lambda increases -> the coefficients decrease
predict(ridge.mod,s=50,type="coefficients")[1:40,] # s = lambda value, predict of the coefficients
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# Fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda (s) = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function for a test set
mean((ridge.pred-y.test)^2)
# Two predictions with different (arbitrary) values of lambda:
# lambda --> +OO (10^10) means coefficients close to zero
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
# lambda --> 0 means that Least squares is simply ridge regression.
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((ridge.pred-y.test)^2)
# Same result obtained with lm(), exact = T works again on the model (but not the known model)
# Comparison of the results between lm() and glmnet when lambda=0:
lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients",x=x[train,],y=y[train])[1:40,]
#Best results are obtained with MSE compared to Ridge Regression
#Instead of arbitrary values, we now use method "Cross validation" to estimate lambda:
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam # the best lambda
log(bestlam) # log value of previous lambda
# Prediction of the model with the best value of lambda
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
# Prediction of the coefficients with the best value of lambda
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:40,]
# As expected, none of the coefficients is zero
# Figure that show the variation of the coefficients with different values of lambda
dev.new()
plot(out,label = T, xvar = "lambda")
#####################
####### LASSO #######
# Use the argument alpha = 1 to perform Lasso
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T) # L1 norm
# Figure that show the variation of the coefficients with different values of lambda
dev.new()
plot(lasso.mod,label = T, xvar = "lambda")
# Perform Cross-Validation for estimate the best lambda to minimize "mse test"
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min; print(bestlam);print(log(bestlam)) # the best lambda
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
# Comparison between lm() and lasso-model with lambda = 0
lasso.pred=predict(lasso.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:40,]
lasso.coef
lasso.coef[lasso.coef!=0]
cat("Number of coefficients equal to 0:",sum(lasso.coef==0),"\n")
